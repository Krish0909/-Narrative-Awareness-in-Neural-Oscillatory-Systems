# -*- coding: utf-8 -*-
"""word predict

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/130unTN7kRasqUKWA73-R82Vsvjpei6Wj
"""

import os
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import kagglehub
from torch.optim.lr_scheduler import ReduceLROnPlateau
from transformers import AutoTokenizer, AutoModel
from scipy import signal
from scipy.signal import butter, filtfilt, hilbert
from scipy.spatial.distance import pdist, squareform
import warnings
warnings.filterwarnings('ignore')

print("🧠 NeuroSemantic Graph ESN - FIXED VERSION (No Data Leakage)")
print("=" * 70)

# Fixed CUDA detection
def setup_device():
    """Robust device setup with fallback."""
    if torch.cuda.is_available():
        try:
            # Test CUDA with a small operation
            test_tensor = torch.randn(10, 10).cuda()
            _ = test_tensor @ test_tensor.T
            device = torch.device('cuda')
            print(f"✅ CUDA working! Using: {torch.cuda.get_device_name(0)}")
            print(f"   CUDA Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
        except Exception as e:
            print(f"⚠️ CUDA available but not working: {e}")
            device = torch.device('cpu')
            print("🔄 Falling back to CPU")
    else:
        device = torch.device('cpu')
        print("💻 CUDA not available, using CPU")

    return device

device = setup_device()

# Configuration
class Config:
    def __init__(self):
        # Data parameters
        self.NUM_ELECTRODES = 64
        self.PRE_WORD_CONTEXT = 200   # 200ms before word
        self.POST_WORD_WINDOW = 800   # 800ms after word to capture response
        self.SAMPLING_RATE = 1000
        self.BATCH_SIZE = 32 if device.type == 'cuda' else 16
        self.MAX_SEQUENCES = 5000 if device.type == 'cuda' else 3000

        # Signal processing
        self.GAMMA_LOW = 30
        self.GAMMA_HIGH = 100
        self.BETA_LOW = 13
        self.BETA_HIGH = 30
        self.ALPHA_LOW = 8
        self.ALPHA_HIGH = 13

        # Word processing
        self.WORD_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"
        self.WORD_EMBEDDING_DIM = 128

        # Graph ESN parameters
        self.RESERVOIR_SIZE = 512 if device.type == 'cuda' else 256
        self.SPECTRAL_RADIUS = 0.9
        self.INPUT_SCALING = 0.3
        self.LEAKY_RATE = 0.2
        self.GRAPH_CONNECTIVITY = 0.15

        # Continuous Latency Attention
        self.MAX_LATENCY_MS = 600
        self.LATENCY_RESOLUTION = 50
        self.LATENCY_SIGMA = 30

        # Architecture
        self.HIDDEN_DIM = 128
        self.CROSS_MODAL_DIM = 64
        self.DROPOUT = 0.1

        # Training
        self.EPOCHS = 50 if device.type == 'cuda' else 30
        self.LEARNING_RATE = 1e-3
        self.WEIGHT_DECAY = 1e-4
        self.PATIENCE = 10

config = Config()

# Set seeds for reproducibility
SEED = 42
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed(SEED)
    torch.cuda.manual_seed_all(SEED)
print(f"✅ Random seeds set for {device.type.upper()}")

#----------------------------------------------------------------------
# Enhanced Signal Processing (FIXED - No Data Leakage)
#----------------------------------------------------------------------

def extract_multi_band_features(signal_data, fs=1000):
    """Extract features from multiple frequency bands."""
    features = []

    # Define frequency bands
    bands = [
        ('alpha', config.ALPHA_LOW, config.ALPHA_HIGH),
        ('beta', config.BETA_LOW, config.BETA_HIGH),
        ('gamma', config.GAMMA_LOW, config.GAMMA_HIGH)
    ]

    nyquist = fs / 2

    for band_name, low, high in bands:
        low_norm = low / nyquist
        high_norm = high / nyquist

        b, a = butter(4, [low_norm, high_norm], btype='band')

        band_powers = []
        for i in range(signal_data.shape[1]):
            # Filter signal
            filtered = filtfilt(b, a, signal_data[:, i])

            # Extract power using Hilbert transform
            analytic_signal = hilbert(filtered)
            power = np.abs(analytic_signal) ** 2

            # Smooth with 20ms window
            window = 20
            power = np.convolve(power, np.ones(window)/window, mode='same')
            band_powers.append(power)

        features.append(np.array(band_powers).T)

    # Combine all bands
    combined_features = np.concatenate(features, axis=1)
    print(f"🔬 Extracted {combined_features.shape[1]} features ({len(bands)} bands × {config.NUM_ELECTRODES} electrodes)")

    return combined_features

def create_realistic_electrode_graph(num_electrodes):
    """Create realistic electrode graph with spatial structure."""
    print("🔗 Creating realistic spatial electrode graph...")

    # Create 2D grid layout mimicking real ECoG electrode placement
    grid_size = int(np.ceil(np.sqrt(num_electrodes)))
    positions = []

    for i in range(num_electrodes):
        row = i // grid_size
        col = i % grid_size
        # Add spatial noise for realistic positioning
        x = col + np.random.normal(0, 0.1)
        y = row + np.random.normal(0, 0.1)
        positions.append([x, y])

    positions = np.array(positions)

    # Compute distances
    distances = squareform(pdist(positions))

    # Create sophisticated adjacency matrix
    adjacency = np.zeros_like(distances)

    # Multiple connection strategies for realistic brain connectivity
    for i in range(len(distances)):
        # Local connections (nearest neighbors)
        local_neighbors = min(6, num_electrodes - 1)
        local_indices = np.argsort(distances[i])[1:local_neighbors+1]

        # Long-range connections (skip connections)
        long_range_neighbors = min(3, num_electrodes - 1)
        long_range_indices = np.argsort(distances[i])[-(long_range_neighbors+1):-1]

        # Add weighted connections
        for j in local_indices:
            weight = np.exp(-distances[i, j] / 2.0)  # Exponential decay
            adjacency[i, j] = weight
            adjacency[j, i] = weight

        for j in long_range_indices:
            weight = 0.1 * np.exp(-distances[i, j] / 5.0)  # Weak long-range
            adjacency[i, j] = weight
            adjacency[j, i] = weight

    # Normalize
    row_sums = adjacency.sum(axis=1)
    row_sums[row_sums == 0] = 1  # Avoid division by zero
    adjacency = adjacency / row_sums[:, np.newaxis]

    electrode_names = [f"electrode_{i:02d}" for i in range(num_electrodes)]

    print(f"✅ Created sophisticated graph with {(adjacency > 0).sum():.0f} connections")
    print(f"📊 Average degree: {(adjacency > 0).sum(axis=1).mean():.1f}")

    return adjacency, electrode_names, positions

def download_and_load_data():
    """Download and process data - FIXED VERSION with proper temporal splitting."""
    print("📥 Downloading ECoG dataset...")
    try:
        path = kagglehub.dataset_download("arunramponnambalam/podcast-csv-aligned")
        print(f"✅ Dataset downloaded to: {path}")
    except Exception as e:
        print(f"❌ Error downloading dataset: {e}")
        return None

    # Find data file
    ecog_file = None
    for root, dirs, files in os.walk(path):
        for file in files:
            if 'processed_ecog_data' in file and file.endswith('.csv'):
                ecog_file = os.path.join(root, file)
                break

    if not ecog_file:
        raise FileNotFoundError("Could not find ECoG data file")

    print(f"📂 Loading data from: {ecog_file}")
    # Load more data for robust evaluation
    ecog_data = pd.read_csv(ecog_file, nrows=150000)

    # Extract electrode columns
    electrode_columns = [col for col in ecog_data.columns if col not in ['timestamp', 'word']]
    selected_electrodes = electrode_columns[:config.NUM_ELECTRODES]

    electrode_data = ecog_data[selected_electrodes].values
    word_data = ecog_data['word'].fillna('').values
    timestamps = ecog_data.get('timestamp', np.arange(len(electrode_data))).values

    # Clean data
    valid_mask = ~np.isnan(electrode_data).any(axis=1)
    electrode_data = electrode_data[valid_mask]
    word_data = word_data[valid_mask]
    timestamps = timestamps[valid_mask]

    print(f"✅ Loaded {len(electrode_data)} samples with {config.NUM_ELECTRODES} electrodes")

    return electrode_data, word_data, timestamps, selected_electrodes

def create_word_events_with_temporal_split(neural_features, word_data, timestamps):
    """Create word events and split temporally to avoid data leakage."""

    # Use actual frequent words from dataset
    print("🔍 Analyzing word frequency in dataset...")
    frequent_words = ['humans', 'designed', 'places', 'act', 'monkey', 'animals']

    # Count actual occurrences
    word_counts_actual = {word: 0 for word in frequent_words}
    for word in word_data:
        if isinstance(word, str) and len(word.strip()) > 0:
            word_clean = word.strip().lower()
            if word_clean in frequent_words:
                word_counts_actual[word_clean] += 1

    print("📊 Actual word counts in dataset:")
    for word, count in word_counts_actual.items():
        print(f"   '{word}': {count}")

    # Filter to words with sufficient samples
    min_samples = 50  # Higher threshold for robust evaluation
    target_words = [word for word in frequent_words if word_counts_actual[word] >= min_samples]

    if len(target_words) < 2:
        print("❌ Not enough words with sufficient samples!")
        return None, None, None

    print(f"✅ Using {len(target_words)} words: {target_words}")

    word_to_label = {word: i for i, word in enumerate(target_words)}

    # Find ALL word events first (chronologically ordered)
    print(f"🔍 Finding word events chronologically...")
    all_word_events = []
    word_event_counts = {word: 0 for word in target_words}
    max_per_class = config.MAX_SEQUENCES // len(target_words)

    for i, word in enumerate(word_data):
        if isinstance(word, str) and len(word.strip()) > 0:
            word_clean = word.strip().lower()

            if (word_clean in target_words and
                word_event_counts[word_clean] < max_per_class and
                i >= config.PRE_WORD_CONTEXT and
                i + config.POST_WORD_WINDOW < len(neural_features)):

                all_word_events.append({
                    'word': word_clean,
                    'label': word_to_label[word_clean],
                    'data_index': i,  # Store index for later extraction
                    'timestamp': timestamps[i] if len(timestamps) > i else i
                })

                word_event_counts[word_clean] += 1

    print(f"✅ Found {len(all_word_events)} word events chronologically")

    # TEMPORAL SPLIT - No data leakage
    print("📅 Performing temporal split to avoid data leakage...")

    # Sort by timestamp to ensure temporal order
    all_word_events.sort(key=lambda x: x['timestamp'])

    # Split chronologically: 60% train, 20% val, 20% test
    n_events = len(all_word_events)
    train_end = int(0.6 * n_events)
    val_end = int(0.8 * n_events)

    train_events_meta = all_word_events[:train_end]
    val_events_meta = all_word_events[train_end:val_end]
    test_events_meta = all_word_events[val_end:]

    print(f"📊 Temporal split: Train={len(train_events_meta)}, Val={len(val_events_meta)}, Test={len(test_events_meta)}")

    # Check class distribution in each split
    for split_name, events in [("Train", train_events_meta), ("Val", val_events_meta), ("Test", test_events_meta)]:
        if events:
            labels = [e['label'] for e in events]
            unique_labels, counts = np.unique(labels, return_counts=True)
            print(f"   {split_name} distribution: {dict(zip([target_words[l] for l in unique_labels], counts))}")

    return train_events_meta, val_events_meta, test_events_meta, target_words, word_to_label

#----------------------------------------------------------------------
# Data Processing Pipeline (FIXED - No Leakage)
#----------------------------------------------------------------------

class DataProcessor:
    """Handles all data preprocessing without leakage."""

    def __init__(self):
        self.scaler = StandardScaler()
        self.electrode_graph = None
        self.electrode_names = None
        self.fitted = False

    def fit_on_train_data(self, train_electrode_data):
        """Fit preprocessing only on training data."""
        print("🔧 Fitting preprocessing on training data only...")

        # Fit scaler ONLY on training data
        self.scaler.fit(train_electrode_data)

        # Create electrode graph (doesn't use data, so no leakage)
        self.electrode_graph, self.electrode_names, _ = create_realistic_electrode_graph(
            train_electrode_data.shape[1]
        )

        self.fitted = True
        print("✅ Preprocessing fitted on training data only")

    def transform_data(self, electrode_data):
        """Transform data using fitted preprocessing."""
        if not self.fitted:
            raise ValueError("Must fit on training data first!")

        # Scale using training statistics
        scaled_data = self.scaler.transform(electrode_data)

        # Extract features
        neural_features = extract_multi_band_features(scaled_data, config.SAMPLING_RATE)

        return neural_features

    def create_sequences_from_events(self, neural_features, events_meta):
        """Create neural sequences from event metadata."""
        sequences = []

        for event_meta in events_meta:
            i = event_meta['data_index']

            # Ensure we have enough context
            if (i >= config.PRE_WORD_CONTEXT and
                i + config.POST_WORD_WINDOW < len(neural_features)):

                # Extract features around word presentation
                pre_context = neural_features[i - config.PRE_WORD_CONTEXT:i]
                post_context = neural_features[i:i + config.POST_WORD_WINDOW]

                # Combine pre and post context
                full_context = np.concatenate([pre_context, post_context], axis=0)

                sequences.append({
                    'word': event_meta['word'],
                    'label': event_meta['label'],
                    'neural_sequence': full_context
                })

        print(f"✅ Created {len(sequences)} neural sequences")
        return sequences

#----------------------------------------------------------------------
# Word Embeddings (Fixed for efficiency)
#----------------------------------------------------------------------

class WordEmbedder(nn.Module):
    """Word embedder with caching for efficiency."""

    def __init__(self):
        super().__init__()

        print(f"🧠 Loading language model: {config.WORD_MODEL_NAME}")
        self.tokenizer = AutoTokenizer.from_pretrained(config.WORD_MODEL_NAME)
        self.language_model = AutoModel.from_pretrained(config.WORD_MODEL_NAME)

        # Freeze language model to save computation
        for param in self.language_model.parameters():
            param.requires_grad = False

        # Move to device
        self.language_model = self.language_model.to(device)

        # Semantic processing layers
        self.semantic_processor = nn.Sequential(
            nn.Linear(self.language_model.config.hidden_size, config.WORD_EMBEDDING_DIM),
            nn.LayerNorm(config.WORD_EMBEDDING_DIM),
            nn.GELU(),
            nn.Dropout(config.DROPOUT),
            nn.Linear(config.WORD_EMBEDDING_DIM, config.WORD_EMBEDDING_DIM),
        )

        # Cache for efficient processing
        self.embedding_cache = {}
        self._embeddings_computed = False

    def _ensure_embeddings_computed(self):
        """Pre-compute embeddings for target words."""
        if not self._embeddings_computed:
            print("⚡ Pre-computing word embeddings...")
            target_words = ['humans', 'designed', 'places', 'act', 'monkey', 'animals']

            with torch.no_grad():
                for word in target_words:
                    inputs = self.tokenizer(word, return_tensors="pt", padding=True, truncation=True)
                    inputs = {k: v.to(device) for k, v in inputs.items()}

                    outputs = self.language_model(**inputs)
                    raw_embedding = outputs.last_hidden_state.mean(dim=1).squeeze()
                    processed_embedding = self.semantic_processor(raw_embedding)
                    self.embedding_cache[word] = processed_embedding.detach()

            self._embeddings_computed = True
            print(f"✅ Pre-computed embeddings for {len(target_words)} words")

    def forward(self, words):
        """Process batch of words using cached embeddings."""
        self._ensure_embeddings_computed()

        embeddings = []
        for word in words:
            if word in self.embedding_cache:
                embeddings.append(self.embedding_cache[word])
            else:
                # Fallback for unseen words
                with torch.no_grad():
                    inputs = self.tokenizer(word, return_tensors="pt", padding=True, truncation=True)
                    inputs = {k: v.to(device) for k, v in inputs.items()}

                    outputs = self.language_model(**inputs)
                    raw_embedding = outputs.last_hidden_state.mean(dim=1).squeeze()
                processed_embedding = self.semantic_processor(raw_embedding)
                embeddings.append(processed_embedding)
                self.embedding_cache[word] = processed_embedding.detach()

        return torch.stack(embeddings)

#----------------------------------------------------------------------
# Graph Echo State Network (Optimized)
#----------------------------------------------------------------------

class GraphEchoStateNetwork(nn.Module):
    """Graph-based Echo State Network."""

    def __init__(self, input_size, reservoir_size, graph_adjacency):
        super().__init__()

        self.input_size = input_size
        self.reservoir_size = reservoir_size
        self.num_electrodes = graph_adjacency.shape[0]

        # Convert graph adjacency to tensor
        self.register_buffer('graph_adjacency', torch.FloatTensor(graph_adjacency))

        # ESN reservoir weights (fixed)
        self.register_buffer('W_in', self._initialize_input_weights())
        self.register_buffer('W_res', self._initialize_reservoir_weights())

        # Learnable graph processing
        self.graph_conv = nn.Sequential(
            nn.Linear(self.num_electrodes, self.num_electrodes),
            nn.GELU(),
            nn.Linear(self.num_electrodes, self.num_electrodes)
        )

        # Projection layers
        self.input_projection = nn.Linear(input_size, reservoir_size)
        self.electrode_to_reservoir = nn.Linear(self.num_electrodes, reservoir_size)

        # Learnable parameters
        self.input_scaling = nn.Parameter(torch.tensor(config.INPUT_SCALING))
        self.leaky_rate = nn.Parameter(torch.tensor(config.LEAKY_RATE))

        print(f"🔧 Graph ESN: {input_size} → {reservoir_size}")

    def _initialize_input_weights(self):
        """Initialize sparse input weights."""
        W_in = torch.zeros(self.reservoir_size, self.reservoir_size)
        mask = torch.rand(self.reservoir_size, self.reservoir_size) < 0.1
        W_in[mask] = torch.randn(mask.sum()) * 0.5
        return W_in

    def _initialize_reservoir_weights(self):
        """Initialize reservoir weights with spectral radius control."""
        W_res = torch.zeros(self.reservoir_size, self.reservoir_size)

        # Sparse connectivity
        num_connections = int(config.GRAPH_CONNECTIVITY * self.reservoir_size * self.reservoir_size)
        for _ in range(num_connections):
            i = np.random.randint(0, self.reservoir_size)
            j = np.random.randint(0, self.reservoir_size)
            if i != j:
                W_res[i, j] = np.random.normal(0, 0.3)

        # Scale by spectral radius
        eigenvalues = torch.linalg.eigvals(W_res)
        spectral_radius = torch.max(torch.abs(eigenvalues)).item()
        if spectral_radius > 1e-6:
            W_res = W_res * (config.SPECTRAL_RADIUS / spectral_radius)

        return W_res

    def _graph_convolution(self, electrode_signals):
        """Apply graph convolution."""
        # Spatial filtering using graph adjacency
        spatial_filtered = torch.matmul(electrode_signals, self.graph_adjacency)
        # Learned transformation
        graph_processed = self.graph_conv(spatial_filtered)
        return graph_processed

    def forward(self, x_sequence):
        """Process sequence through Graph ESN."""
        batch_size, seq_len, _ = x_sequence.shape

        # Initialize hidden state
        h = torch.zeros(batch_size, self.reservoir_size, device=x_sequence.device)

        # Process sequence (optimize for efficiency)
        for t in range(seq_len):
            x_t = x_sequence[:, t, :]

            # Graph-based spatial processing
            x_graph = self._graph_convolution(x_t[:, :self.num_electrodes])
            x_projected = self.electrode_to_reservoir(x_graph)

            # Input contribution
            input_contrib = torch.matmul(x_projected, self.W_in.T) * torch.sigmoid(self.input_scaling)

            # Recurrent contribution
            recurrent_contrib = torch.matmul(h, self.W_res.T)

            # Combine and activate
            combined = input_contrib + recurrent_contrib
            activated = torch.tanh(combined)

            # Leaky integration
            alpha = torch.sigmoid(self.leaky_rate)
            h = (1 - alpha) * h + alpha * activated

        return h

#----------------------------------------------------------------------
# Continuous Latency Attention
#----------------------------------------------------------------------

class ContinuousLatencyAttention(nn.Module):
    """Novel continuous latency attention mechanism."""

    def __init__(self, word_dim, neural_dim):
        super().__init__()

        # Continuous latency prediction
        self.latency_predictor = nn.Sequential(
            nn.Linear(word_dim + neural_dim, config.HIDDEN_DIM),
            nn.GELU(),
            nn.Dropout(config.DROPOUT),
            nn.Linear(config.HIDDEN_DIM, config.HIDDEN_DIM),
            nn.GELU(),
            nn.Linear(config.HIDDEN_DIM, 1),
            nn.Sigmoid()
        )

        # Latency attention weights generator
        self.attention_generator = nn.Sequential(
            nn.Linear(word_dim + neural_dim + 1, config.HIDDEN_DIM),
            nn.GELU(),
            nn.Linear(config.HIDDEN_DIM, config.LATENCY_RESOLUTION),
            nn.Softmax(dim=1)
        )

        # Create latency time grid
        latency_grid = torch.linspace(0, config.MAX_LATENCY_MS, config.LATENCY_RESOLUTION)
        self.register_buffer('latency_grid', latency_grid)

        print(f"🎯 Continuous Latency Attention: {config.LATENCY_RESOLUTION} time steps")

    def forward(self, word_embedding, neural_state):
        """Generate continuous latency attention."""
        # Combine features
        combined = torch.cat([word_embedding, neural_state], dim=1)

        # Predict continuous latency
        latency_pred = self.latency_predictor(combined)
        latency_ms = latency_pred * config.MAX_LATENCY_MS

        # Generate continuous attention weights using Gaussian kernel
        latency_expanded = latency_ms.unsqueeze(1)
        grid_expanded = self.latency_grid.unsqueeze(0).unsqueeze(0)

        # Gaussian attention kernel
        sigma = config.LATENCY_SIGMA
        attention_weights = torch.exp(-0.5 * ((latency_expanded - grid_expanded) / sigma) ** 2)
        attention_weights = attention_weights / attention_weights.sum(dim=2, keepdim=True)

        # Generate additional contextual attention
        combined_with_latency = torch.cat([combined, latency_pred], dim=1)
        contextual_attention = self.attention_generator(combined_with_latency)

        # Combine attention mechanisms
        final_attention = 0.7 * attention_weights.squeeze(1) + 0.3 * contextual_attention

        return final_attention, latency_ms.squeeze(1)

#----------------------------------------------------------------------
# Cross-Modal Fusion
#----------------------------------------------------------------------

class CrossModalFusion(nn.Module):
    """Cross-modal fusion mechanism."""

    def __init__(self, word_dim, neural_dim, output_dim):
        super().__init__()

        # Projection layers
        self.word_proj = nn.Sequential(
            nn.Linear(word_dim, output_dim),
            nn.LayerNorm(output_dim),
            nn.GELU(),
            nn.Dropout(config.DROPOUT)
        )

        self.neural_proj = nn.Sequential(
            nn.Linear(neural_dim, output_dim),
            nn.LayerNorm(output_dim),
            nn.GELU(),
            nn.Dropout(config.DROPOUT)
        )

        # Fusion layers
        self.fusion_network = nn.Sequential(
            nn.Linear(output_dim * 2, output_dim),
            nn.LayerNorm(output_dim),
            nn.GELU(),
            nn.Dropout(config.DROPOUT),
            nn.Linear(output_dim, output_dim)
        )

        # Gating mechanism
        self.gate = nn.Sequential(
            nn.Linear(output_dim * 2, output_dim),
            nn.Sigmoid()
        )

    def forward(self, word_embedding, neural_state):
        """Cross-modal fusion."""
        # Project to common dimensions
        word_proj = self.word_proj(word_embedding)
        neural_proj = self.neural_proj(neural_state)

        # Simple cross-attention for efficiency
        attended_neural = neural_proj + 0.1 * word_proj
        attended_word = word_proj + 0.1 * neural_proj

        # Concatenate and fuse
        combined = torch.cat([attended_word, attended_neural], dim=1)
        fused = self.fusion_network(combined)

        # Apply gating
        gate_weights = self.gate(combined)
        gated_fused = fused * gate_weights

        return gated_fused

#----------------------------------------------------------------------
# Main Model
#----------------------------------------------------------------------

class NeuroSemanticGraphESN(nn.Module):
    """Fixed NeuroSemantic Graph ESN without data leakage."""

    def __init__(self, electrode_graph, num_classes):
        super().__init__()

        print("🏗️ Building Fixed NeuroSemantic Graph ESN...")

        # Core components
        self.word_embedder = WordEmbedder()

        self.graph_esn = GraphEchoStateNetwork(
            input_size=config.NUM_ELECTRODES * 3,  # 3 frequency bands
            reservoir_size=config.RESERVOIR_SIZE,
            graph_adjacency=electrode_graph
        )

        self.cross_modal_fusion = CrossModalFusion(
            word_dim=config.WORD_EMBEDDING_DIM,
            neural_dim=config.RESERVOIR_SIZE,
            output_dim=config.CROSS_MODAL_DIM
        )

        self.latency_attention = ContinuousLatencyAttention(
            word_dim=config.WORD_EMBEDDING_DIM,
            neural_dim=config.RESERVOIR_SIZE
        )

        # Classification head
        self.classifier = nn.Sequential(
            nn.Linear(config.CROSS_MODAL_DIM + config.LATENCY_RESOLUTION, config.HIDDEN_DIM),
            nn.LayerNorm(config.HIDDEN_DIM),
            nn.GELU(),
            nn.Dropout(config.DROPOUT),
            nn.Linear(config.HIDDEN_DIM, config.HIDDEN_DIM),
            nn.GELU(),
            nn.Dropout(config.DROPOUT),
            nn.Linear(config.HIDDEN_DIM, num_classes)
        )

        print(f"✅ Fixed Architecture built for {num_classes} classes!")
        print(f"   Novel features: Continuous Latency Attention, Graph ESN, Cross-Modal Fusion")
        print(f"   NO DATA LEAKAGE: Temporal splitting + proper preprocessing")

    def forward(self, pre_context, words, return_analysis=False):
        """Forward pass."""
        batch_size = pre_context.shape[0]

        # 1. Encode words semantically
        word_embeddings = self.word_embedder(words)

        # 2. Process neural context through Graph ESN
        neural_state = self.graph_esn(pre_context)

        # 3. Continuous latency attention
        latency_attention, predicted_latency = self.latency_attention(word_embeddings, neural_state)

        # 4. Cross-modal fusion
        fused_representation = self.cross_modal_fusion(word_embeddings, neural_state)

        # 5. Combine with latency attention for classification
        combined_features = torch.cat([fused_representation, latency_attention], dim=1)
        logits = self.classifier(combined_features)

        if return_analysis:
            analysis_info = {
                'word_embeddings': word_embeddings,
                'neural_states': neural_state,
                'fused_representations': fused_representation,
                'latency_attention': latency_attention,
                'predicted_latency': predicted_latency
            }
            return logits, analysis_info

        return logits

#----------------------------------------------------------------------
# Dataset
#----------------------------------------------------------------------

class WordClassificationDataset(Dataset):
    """Dataset for word classification."""

    def __init__(self, word_events):
        self.word_events = word_events

    def __len__(self):
        return len(self.word_events)

    def __getitem__(self, idx):
        event = self.word_events[idx]

        return {
            'neural_sequence': torch.FloatTensor(event['neural_sequence']),
            'label': torch.LongTensor([event['label']]).squeeze(),
            'word': event['word']
        }

#----------------------------------------------------------------------
# Training Functions
#----------------------------------------------------------------------

def train_epoch(model, train_loader, optimizer, criterion, epoch):
    """Train for one epoch."""
    model.train()
    total_loss = 0.0
    correct = 0
    total = 0

    progress_bar = tqdm(train_loader, desc=f"Training Epoch {epoch+1}")

    for batch in progress_bar:
        neural_seq = batch['neural_sequence'].to(device)
        labels = batch['label'].to(device)
        words = batch['word']

        # Forward pass
        logits = model(neural_seq, words)
        loss = criterion(logits, labels)

        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()

        # Calculate accuracy
        _, predicted = torch.max(logits.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

        total_loss += loss.item()
        accuracy = 100 * correct / total

        progress_bar.set_postfix({
            'loss': f'{loss.item():.4f}',
            'acc': f'{accuracy:.2f}%'
        })

    return total_loss / len(train_loader), accuracy

def evaluate(model, val_loader, criterion, target_words):
    """Evaluate model."""
    model.eval()

    # --- FIX: Handle the case where the dataloader is empty ---
    if not val_loader or len(val_loader.dataset) == 0:
        print("⚠️ Warning: Evaluation loader is empty. Skipping evaluation.")
        return {
            'loss': 0.0,
            'accuracy': 0.0,
            'predictions': [],
            'labels': [],
            'words': [],
            'analysis': [],
            'classification_report': {},
            'confusion_matrix': np.array([])
        }
    # --- END FIX ---

    total_loss = 0.0
    all_predictions = []
    all_labels = []
    all_words = []
    all_analysis = []

    with torch.no_grad():
        for batch in tqdm(val_loader, desc="Evaluating"):
            neural_seq = batch['neural_sequence'].to(device)
            labels = batch['label'].to(device)
            words = batch['word']

            logits, analysis = model(neural_seq, words, return_analysis=True)
            loss = criterion(logits, labels)

            total_loss += loss.item()

            _, predicted = torch.max(logits.data, 1)
            all_predictions.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_words.extend(words)
            all_analysis.append(analysis)

    # Calculate metrics
    accuracy = accuracy_score(all_labels, all_predictions)

    # Classification report
    unique_labels = np.unique(all_labels)
    existing_target_words = [target_words[i] for i in unique_labels]

    report = classification_report(all_labels, all_predictions,
                                 target_names=existing_target_words,
                                 labels=unique_labels,
                                 output_dict=True, zero_division=0)

    # Confusion matrix
    cm = confusion_matrix(all_labels, all_predictions)

    return {
        'loss': total_loss / len(val_loader),
        'accuracy': accuracy,
        'predictions': all_predictions,
        'labels': all_labels,
        'words': all_words,
        'analysis': all_analysis,
        'classification_report': report,
        'confusion_matrix': cm
    }

#----------------------------------------------------------------------
# Visualization
#----------------------------------------------------------------------

def plot_fixed_analysis(results, target_words, epoch):
    """Create analysis plots for fixed version."""

    fig, axes = plt.subplots(2, 3, figsize=(18, 12))

    # Get unique labels and corresponding words
    unique_labels = np.unique(results['labels'])
    existing_words = [target_words[i] for i in unique_labels if i < len(target_words)]

    # 1. Confusion Matrix
    cm = results['confusion_matrix']
    if cm.size > 0:
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=existing_words, yticklabels=existing_words,
                    ax=axes[0, 0])
    axes[0, 0].set_title(f'Confusion Matrix - Epoch {epoch}', fontsize=14, fontweight='bold')
    axes[0, 0].set_xlabel('Predicted')
    axes[0, 0].set_ylabel('True')

    # 2. Per-class Performance
    report = results['classification_report']
    if existing_words and len(existing_words) > 1:
        metrics = ['precision', 'recall', 'f1-score']
        x = np.arange(len(existing_words))
        width = 0.25

        for i, metric in enumerate(metrics):
            values = [report[word][metric] for word in existing_words if word in report]
            axes[0, 1].bar(x + i*width, values, width,
                          label=metric.title(), alpha=0.8)

        axes[0, 1].set_xlabel('Words')
        axes[0, 1].set_ylabel('Score')
        axes[0, 1].set_title('Per-Class Performance (No Leakage)', fontsize=14, fontweight='bold')
        axes[0, 1].set_xticks(x + width)
        axes[0, 1].set_xticklabels(existing_words, rotation=45)
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)

    # 3. Neural State PCA
    if results['analysis']:
        neural_states = torch.cat([batch['neural_states'].cpu() for batch in results['analysis']], dim=0)
        labels = np.array(results['labels'])

        pca = PCA(n_components=2)
        states_2d = pca.fit_transform(neural_states.numpy())

        colors = plt.cm.Set3(np.linspace(0, 1, len(existing_words)))
        for i, (word, color) in enumerate(zip(existing_words, colors)):
            if i in unique_labels:
                mask = labels == i
                if mask.sum() > 0:
                    axes[0, 2].scatter(states_2d[mask, 0], states_2d[mask, 1],
                                     c=[color], label=word, alpha=0.7, s=30)

        axes[0, 2].set_title(f'Neural States (PCA) - Epoch {epoch}', fontsize=14, fontweight='bold')
        axes[0, 2].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')
        axes[0, 2].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')
        axes[0, 2].legend()
        axes[0, 2].grid(True, alpha=0.3)

    # 4. Latency Analysis
    if results['analysis']:
        predicted_latencies = torch.cat([batch['predicted_latency'].cpu() for batch in results['analysis']], dim=0)
        labels = np.array(results['labels'])

        # Latency distribution by word
        for i, word in enumerate(existing_words):
            if i in unique_labels:
                word_latencies = predicted_latencies[labels == i].numpy()
                if len(word_latencies) > 0:
                    axes[1, 0].hist(word_latencies, bins=15, alpha=0.6,
                                   label=word, density=True)

        axes[1, 0].set_title('Predicted Latency Distributions (FIXED)', fontsize=14, fontweight='bold')
        axes[1, 0].set_xlabel('Latency (ms)')
        axes[1, 0].set_ylabel('Density')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)

    # 5. Overall Performance Summary
    overall_acc = results['accuracy']
    axes[1, 1].text(0.5, 0.5,
                   f'FIXED VERSION\n\n'
                   f'Overall Accuracy: {overall_acc:.1%}\n\n'
                   f'Classes: {len(existing_words)}\n\n'
                   f'✅ NO DATA LEAKAGE:\n'
                   f'• Temporal splitting\n'
                   f'• Train-only preprocessing\n'
                   f'• Proper validation\n\n'
                   f'Novel Features:\n'
                   f'• Continuous Latency\n'
                   f'• Graph ESN\n'
                   f'• Cross-Modal Fusion',
                   ha='center', va='center', fontsize=12,
                   bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgreen"))
    axes[1, 1].set_xlim(0, 1)
    axes[1, 1].set_ylim(0, 1)
    axes[1, 1].set_title('FIXED Architecture Summary', fontsize=14, fontweight='bold')
    axes[1, 1].axis('off')

    # 6. Class Distribution
    unique_labels_plot, counts = np.unique(results['labels'], return_counts=True)
    class_names = [existing_words[i] if i < len(existing_words) else f'Class_{i}' for i in unique_labels_plot]

    if len(class_names) > 1:
        wedges, texts, autotexts = axes[1, 2].pie(counts, labels=class_names, autopct='%1.1f%%', startangle=90)
        axes[1, 2].set_title('Class Distribution', fontsize=14, fontweight='bold')

    plt.suptitle(f'FIXED NeuroSemantic Graph ESN (No Data Leakage) - Epoch {epoch}',
                 fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.savefig(f'fixed_analysis_epoch_{epoch}.png', dpi=300, bbox_inches='tight')
    plt.show()

    # Print detailed results
    print(f"\n📊 FIXED Results - Epoch {epoch}:")
    print(f"   Overall Accuracy: {overall_acc:.1%}")
    print(f"   ✅ NO DATA LEAKAGE: Temporal splitting + proper preprocessing")

    if 'macro avg' in report:
        print(f"   Macro F1-Score: {report['macro avg']['f1-score']:.3f}")

#----------------------------------------------------------------------
# Main Function (FIXED)
#----------------------------------------------------------------------

def main():
    """Fixed main function without data leakage."""
    print("\n🧠 NeuroSemantic Graph ESN - FIXED VERSION")
    print("=" * 70)
    print("✅ FIXED: No data leakage, temporal splitting, proper preprocessing")
    print(f"🚀 Running on: {device.type.upper()}")
    print("=" * 70)

    try:
        # 1. Load raw data
        result = download_and_load_data()
        if result is None:
            return

        electrode_data, word_data, timestamps, electrode_names = result

        # 2. Create word events and split temporally (NO LEAKAGE)
        events_result = create_word_events_with_temporal_split(
            electrode_data, word_data, timestamps
        )
        if events_result[0] is None:
            return

        train_events_meta, val_events_meta, test_events_meta, target_words, word_to_label = events_result

        # 3. Initialize data processor
        processor = DataProcessor()

        # 4. Extract data for each split (maintaining temporal order)
        train_indices = [e['data_index'] for e in train_events_meta]
        val_indices = [e['data_index'] for e in val_events_meta]
        test_indices = [e['data_index'] for e in test_events_meta]

        train_electrode_data = electrode_data[min(train_indices):max(train_indices)+1]
        val_electrode_data = electrode_data[min(val_indices):max(val_indices)+1]
        test_electrode_data = electrode_data[min(test_indices):max(test_indices)+1]

        # 5. Fit preprocessing ONLY on training data
        processor.fit_on_train_data(train_electrode_data)

        # 6. Transform data using training-fitted preprocessing
        print("🔄 Transforming data with training-fitted preprocessing...")
        train_neural_features = processor.transform_data(train_electrode_data)
        val_neural_features = processor.transform_data(val_electrode_data)
        test_neural_features = processor.transform_data(test_electrode_data)

        # 7. Adjust indices for the extracted ranges
        train_base_idx = min(train_indices)
        val_base_idx = min(val_indices)
        test_base_idx = min(test_indices)

        for event in train_events_meta:
            event['data_index'] = event['data_index'] - train_base_idx
        for event in val_events_meta:
            event['data_index'] = event['data_index'] - val_base_idx
        for event in test_events_meta:
            event['data_index'] = event['data_index'] - test_base_idx

        # 8. Create sequences from metadata (NO LEAKAGE)
        train_events = processor.create_sequences_from_events(train_neural_features, train_events_meta)
        val_events = processor.create_sequences_from_events(val_neural_features, val_events_meta)
        test_events = processor.create_sequences_from_events(test_neural_features, test_events_meta)

        print(f"📊 Final data split: Train={len(train_events)}, Val={len(val_events)}, Test={len(test_events)}")

        # 9. Create datasets and loaders
        train_dataset = WordClassificationDataset(train_events)
        val_dataset = WordClassificationDataset(val_events)
        test_dataset = WordClassificationDataset(test_events)

        train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE, shuffle=False)

        num_classes = len(target_words)
        print(f"🎯 Classification task: {num_classes} classes: {target_words}")

        # 10. Create model
        model = NeuroSemanticGraphESN(
            electrode_graph=processor.electrode_graph,
            num_classes=num_classes
        ).to(device)

        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"📊 Model parameters: {total_params:,} total, {trainable_params:,} trainable")

        # 11. Training setup
        criterion = nn.CrossEntropyLoss()
        optimizer = torch.optim.AdamW(model.parameters(), lr=config.LEARNING_RATE,
                                    weight_decay=config.WEIGHT_DECAY)
        scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.8, patience=5, verbose=True)

        best_accuracy = 0.0
        patience_counter = 0

        print("\n🚀 Starting FIXED training (no data leakage)...")

        # 12. Training loop
        for epoch in range(config.EPOCHS):
            print(f"\n{'='*60}")
            print(f"Epoch {epoch+1}/{config.EPOCHS}")
            print('='*60)

            # Train
            train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, epoch)

            # Validate
            val_results = evaluate(model, val_loader, criterion, target_words)
            val_acc = val_results['accuracy'] * 100

            scheduler.step(val_acc)

            print(f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%")
            print(f"Val Loss: {val_results['loss']:.4f}, Val Acc: {val_acc:.2f}%")

            # Analysis every 10 epochs
            if (epoch + 1) % 10 == 0:
                plot_fixed_analysis(val_results, target_words, epoch + 1)

            # Early stopping
            if val_acc > best_accuracy:
                best_accuracy = val_acc
                patience_counter = 0
                print(f"✅ New best accuracy: {best_accuracy:.2f}%")
                torch.save(model.state_dict(), 'best_fixed_model.pt')
            else:
                patience_counter += 1
                if patience_counter >= config.PATIENCE:
                    print("⏰ Early stopping!")
                    break

        # 13. Final test evaluation
        print("\n" + "="*70)
        print("FINAL FIXED TEST EVALUATION (NO DATA LEAKAGE)")
        print("="*70)

        model.load_state_dict(torch.load('best_fixed_model.pt'))
        test_results = evaluate(model, test_loader, criterion, target_words)

        plot_fixed_analysis(test_results, target_words, "FINAL")

        print(f"\n🏆 FINAL FIXED RESULTS:")
        print(f"   Best Validation Accuracy: {best_accuracy:.2f}%")
        print(f"   Final Test Accuracy: {test_results['accuracy']*100:.2f}%")
        print(f"   ✅ NO DATA LEAKAGE:")
        print(f"     • Temporal data splitting")
        print(f"     • Training-only preprocessing")
        print(f"     • Proper cross-validation")
        print(f"   🔬 Novel Architecture Features:")
        print(f"     • Continuous Latency Attention")
        print(f"     • Graph Echo State Networks")
        print(f"     • Cross-Modal Fusion")

        return model, test_results

    except Exception as e:
        print(f"❌ Error: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    result = main()

