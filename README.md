# Neural State Space Mapping: Decoding How the Brain Thinks  

## The Big Idea  

We can decode images and words of what people are thinking from neural activity using neural networks.  
But what if we try to do something far more unhinged—figure out how the brain thinks?  

What if we could map how the different parts of our brain coordinate traversal through this mental information space to make sense of the world—each person with their unique nuances and complexities?  

---

## What We Want to Map  

We want to map how the different cortical populations coordinate the integration of information in this state space to create the perception of complex concepts.  

In other words, how must our cortical populations interact with each other from an information representation perspective to reliably encode complex dynamics in stimuli?  

---

## Our Approach  

### The Core Method  

We intend to use contrastive learning with a bespoke Graph-State Space Model we have built to create a shared embedding space between:  

- The stimulus the brain is processing (semantics, multimodal relationships, for example)  
- The continuous state space in which neural activity encodes information  

Then we can actually map how our cortex coordinates thought through this embedding space.  

### Our Key Insight  

Here’s how our thought process goes:  

- It has been shown that there is a high degree of geometric alignment between the neural state space and that of foundational model embeddings.  
- If that is the case, can we leverage the complex relationships in semantics (often encoded high-dimensionally in the embedding latent space of foundational models) to act as a stencil to look at how neural activity might execute these very functions?  

We can model how the neural activity between different cortical populations must have an effect in their representational spaces so as to reliably model the state space of the world the brain is trying to comprehend.  

---

## What We're Modeling  

We want to use state space modeling at high temporal resolution and multi-perspective contrastive learning to model how different cortical populations evolve their internal state by interacting with each other:  

- What it looks like in terms of neural activity  
- How binding mechanistically happens in the state space  

The cortical code for interaction must be high-dimensional—so can we get a peek into them?  

---

## What We Hope to Achieve  

- Map individual cognitive architectures and their unique organizational principles  
- Understand how different cortical populations construct different aspects of meaning from the same stimuli  
- Bridge the gap between neural activity and subjective conscious experience  
- Move beyond binding simple perceptual features (like color and shape) to complex cognitive binding  

---

## Our Recording Methods  

We want to look at these structures and mechanistic accounts of binding using strictly:  

- **ECoG** – high spatial resolution direct cortical access  
- **MEG** – whole-brain coverage with millisecond temporal precision  

---

## Why This Might Actually Work  

This may sound like overambition by a bunch of college grads—but we think it might just be possible.  

- Most evidence so far comes from binding simple perceptual features like color and shape.  
- The geometric alignment between neural and semantic spaces gives us a new way in.  
- Instead of trying to decode binding mechanisms directly from neural activity, we’re using the known structure of semantic relationships to guide the discovery.  

We’re essentially using **meaning itself as a scaffold for learning how neural activity might have to bind information with the same amount of reliability.**  
